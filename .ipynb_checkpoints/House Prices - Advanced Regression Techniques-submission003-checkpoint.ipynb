{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Importing neccessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "sns.set_style('darkgrid')\n",
    "import scipy.stats as stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pycaret'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpycaret\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mregression\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'pycaret'"
     ]
    }
   ],
   "source": [
    "from pycaret.regression import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"E:/00-Data Science and Data Analysis/ML Projects/02-House Prices - Advanced Regression Techniques/dataset/train.csv\")\n",
    "test = pd.read_csv(\"E:/00-Data Science and Data Analysis/ML Projects/02-House Prices - Advanced Regression Techniques/dataset/test.csv\")\n",
    "submission_sample = pd.read_csv(\"E:/00-Data Science and Data Analysis/ML Projects/02-House Prices - Advanced Regression Techniques/dataset/sample_submission.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "concatenating train and test data frame into one data frame to perform data cleaning ((column wise)) to both data frames once so the data cleaning process would be the same for both data frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.concat([train.drop(train.columns[[0,-1]],axis=1),test.drop(test.columns[0],axis=1)],axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.reset_index(inplace=True,drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Manualy searching for numeric columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical_columns_lst ='LotFrontage,LotArea,OverallQual,OverallCond,YearBuilt,YearRemodAdd,MasVnrArea,BsmtFinSF1,BsmtFinSF2,BsmtUnfSF,TotalBsmtSF,1stFlrSF,2ndFlrSF,LowQualFinSF,GrLivArea,BsmtFullBath,BsmtHalfBath,FullBath,HalfBath,BedroomAbvGr,KitchenAbvGr,TotRmsAbvGrd,Fireplaces,GarageYrBlt,GarageCars,GarageArea,WoodDeckSF,OpenPorchSF,EnclosedPorch,3SsnPorch,ScreenPorch,PoolArea,MiscVal'.split(',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(numerical_columns_lst)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### from the text file we specify and collect the categorical features in the list below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_columns_lst = 'MSSubClass,MSZoning,Street,Alley,LotShape,LandContour,Utilities,LotConfig,LandSlope,Neighborhood,Condition1,Condition2,BldgType,HouseStyle,RoofStyle,RoofMatl,Exterior1st,Exterior2nd,MasVnrType,ExterQual,ExterCond,Foundation,BsmtQual,BsmtCond,BsmtExposure,BsmtFinType1,BsmtFinType2,Heating,HeatingQC,CentralAir,Electrical,KitchenQual,Functional,FireplaceQu,GarageType,GarageFinish,GarageQual,GarageCond,PavedDrive,PoolQC,Fence,MiscFeature,MoSold,YrSold,SaleType,SaleCondition'.split(',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(categorical_columns_lst)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### from the text file we specify and collect the features that have 'na' or 'none' as a category in them in the list below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "na_categories_lst = 'Alley,BsmtQual,BsmtCond,BsmtExposure,BsmtFinType1,BsmtFinType2,FireplaceQu,GarageType,GarageFinish,GarageQual,GarageCond,PoolQC,Fence,MiscFeature,MasVnrType'.split(',')\n",
    "len(na_categories_lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_na_categories = list(set(categorical_columns_lst) - set(na_categories_lst))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(non_na_categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for cat in na_categories_lst:\n",
    "    print(cat,':',data[cat].isna().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Filling the na values in the non-na categorical columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for cat in non_na_categories:\n",
    "    print(cat , ' : ',data[cat].isna().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### filling the na values with the mode of each column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for cat in non_na_categories:\n",
    "    data[cat] = data[cat].fillna(data[cat].mode()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for cat in non_na_categories:\n",
    "    print(cat , ' : ',data[cat].isna().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Now that all na values in the non_na_categorical columns have been filled with the proper values, we will start working on the other categorical columns without na as a category"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### developing a dataframe for the categrical columns with:\n",
    "#### total number of categories per feature\n",
    "#### no of na values in each feature\n",
    "#### is this feature an na_categorical_column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_cat = []\n",
    "num_na = []\n",
    "na_cat = []\n",
    "for cat in categorical_columns_lst:\n",
    "    # num_cat will be a list including the number of categories in the data data frame for each cat column/feature\n",
    "    # within the dataframe\n",
    "    num_cat.append(len(data[cat].unique()))\n",
    "    #num_na will be a list including the number of na values within each coulm/feature in the categorical features in\n",
    "    #the data dataframe\n",
    "    num_na.append(data[cat].isna().sum())\n",
    "    # na_cat will be a list zeros and ones where 0 corresponds to a non_na_category and 1 corresponsd to an na_category\n",
    "    if cat in na_categories_lst:\n",
    "        na_cat.append(1)\n",
    "    else:\n",
    "        na_cat.append(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_num = pd.DataFrame(data={'num_categoreis':num_cat,'num_na':num_na,'na_cat':na_cat},index=categorical_columns_lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "cat_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_num.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data[categorical_columns_lst].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Filling the na values in the numerical columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,12))\n",
    "sns.heatmap(data[numerical_columns_lst].isna())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for cat in numerical_columns_lst:\n",
    "    if data[cat].isna().sum() > 0 :\n",
    "        print(cat, ' : ', data[cat].isna().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### for numeric features related to basement above where na value is present we will fill it with 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[data['BsmtFinSF1'].isna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data.loc[data['BsmtFullBath'].isna(),['BsmtFinSF1','BsmtFinSF2','BsmtUnfSF','TotalBsmtSF','BsmtFullBath','BsmtHalfBath']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.loc[data['BsmtFullBath'].isna(),['BsmtFinSF1','BsmtFinSF2','BsmtUnfSF','TotalBsmtSF','BsmtFullBath','BsmtHalfBath']]= data.loc[data['BsmtFullBath'].isna(),['BsmtFinSF1','BsmtFinSF2','BsmtUnfSF','TotalBsmtSF','BsmtFullBath','BsmtHalfBath']].fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data.iloc[[2120,2188]][['BsmtFinSF1','BsmtFinSF2','BsmtUnfSF','TotalBsmtSF','BsmtFullBath','BsmtHalfBath']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for cat in numerical_columns_lst:\n",
    "    if data[cat].isna().sum() > 0 :\n",
    "        print(cat, ' : ', data[cat].isna().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "garage = ['GarageType','GarageFinish','GarageYrBlt','GarageCars','GarageArea','GarageQual','GarageCond']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### For features related to garage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "data[garage].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking the total number of na values in garage related columns\n",
    "data[garage].isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### for na values in the part of the Data frame above we will fill the na values as follows:\n",
    "#### na values in GarageYrBlt shall be filled with 0 as the concerned houses do not have garages to begin with\n",
    "#### na values in GarageType, GarageFinish, GarageQual, GarageCond shall be filled with 'None'\n",
    "#### na values in GarageArea and GarageCars shall be checked individually since it occurs only once for both,, we could even delete \n",
    "#### the rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking rows where the GarageType feature is na"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.loc[data.GarageType.isna(),garage].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the sums above show equal numbers of na values in most columns which means that these houses have no garages so we can fill \n",
    "# with 'None' in the categorical features ['GarageType','GarageFinish','GarageQual','GarageCond']\n",
    "# and 0 in the numerical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.loc[data.GarageType.isna(),['GarageType', 'GarageFinish', 'GarageQual', 'GarageCond']] = data.loc[data.GarageType.isna(),['GarageType', 'GarageFinish', 'GarageQual', 'GarageCond']].fillna('None')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data.loc[data.GarageType.isna(),garage].isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now let's check the rows where the GarageYrBlt feature is na"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.loc[data.GarageYrBlt.isna(),garage].isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now let's check the two rows where the GarageFinish feature is na"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[data.GarageFinish.isna()][garage].isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data.loc[data.GarageFinish.isna(),garage]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the first house above has a garage but but the second does not seem to have a garage but we will fill both rows with the mode\n",
    "# values for each column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we will leave the GarageFinish feature to be the last to fill since the na values in it are the way to identify these two rows\n",
    "# we will start with GarageQual and GarageCond"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[data.GarageType =='Detchd'].GarageQual.mode()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[data.GarageType =='Detchd'].GarageCond.mode()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.loc[data.GarageFinish.isna(),['GarageQual']] = data.loc[data.GarageFinish.isna(),['GarageQual']].fillna(data[data.GarageType =='Detchd'].GarageQual.mode()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.loc[data.GarageFinish.isna(),['GarageCond']] = data.loc[data.GarageFinish.isna(),['GarageCond']].fillna(data[data.GarageType =='Detchd'].GarageCond.mode()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#checking the two rows to see if the na values above have been filled preperly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.loc[data.GarageFinish.isna(),garage]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we fill GarageArea, GarageCars, and GaraeYrBlt with the mode values for the detahced garages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[data.GarageType =='Detchd'].GarageCars.mode()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[data.GarageType =='Detchd'].GarageArea.mode()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[data.GarageType =='Detchd'].GarageYrBlt.mode()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.loc[data.GarageFinish.isna(),['GarageCars']] = data.loc[data.GarageFinish.isna(),['GarageCars']].fillna(data[data.GarageType =='Detchd'].GarageCars.mode()[0])\n",
    "data.loc[data.GarageFinish.isna(),['GarageArea']] = data.loc[data.GarageFinish.isna(),['GarageArea']].fillna(data[data.GarageType =='Detchd'].GarageArea.mode()[0])\n",
    "data.loc[data.GarageFinish.isna(),['GarageYrBlt']] = data.loc[data.GarageFinish.isna(),['GarageYrBlt']].fillna(data[data.GarageType =='Detchd'].GarageYrBlt.mode()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.loc[data.GarageFinish.isna(),garage]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now we fill GarageFinish with the mode value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[data.GarageType =='Detchd'].GarageFinish.mode()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.GarageFinish = data.GarageFinish.fillna(data[data.GarageType =='Detchd'].GarageFinish.mode()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.iloc[[2126,2576]][garage]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[garage].isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now we check rows where the YrBlt is na to make sure the houses have no garages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "data[data['GarageYrBlt'].isna()][garage]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# yes.. all houses have no garages so we fill these na values with zero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data.GarageYrBlt.fillna(0,inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[data['GarageYrBlt'].isna()][garage]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#now all na values related to garages have been filled properly, we go beck to checkin gthe rest of the coulmns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for cat in numerical_columns_lst:\n",
    "    if data[cat].isna().sum() > 0 :\n",
    "        print(cat, ' : ', data[cat].isna().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we check the Masonry veneer columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data[data['MasVnrArea'].isna()][['MasVnrType','MasVnrArea']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.MasVnrType.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we will fill the na values in the 23 rows above in the MasVnrType column with 'none'\n",
    "# there will be a reamining row where we will separetly check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.loc[data.MasVnrArea.isna(),['MasVnrType']] = data.loc[data.MasVnrArea.isna(),['MasVnrType']].fillna('None')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.loc[data.MasVnrArea.isna(),['MasVnrType']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now we fill the na values in the MasVnrArea with 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.MasVnrArea.fillna(0,inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.MasVnrArea.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.MasVnrType.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.loc[data.MasVnrType.isna(),['MasVnrType','MasVnrArea']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.MasVnrType.value_counts().index[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.MasVnrType.fillna(data.MasVnrType.value_counts().index[1],inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.MasVnrType.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for cat in numerical_columns_lst:\n",
    "    if data[cat].isna().sum() >0:\n",
    "        print(cat, ' : ', data[cat].isna().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will the missing values in this column using KNn regression, meaning to impute the na values as closest to its\n",
    "# similar incidents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data[['LotFrontage']].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[numerical_columns_lst].loc[data['LotFrontage'].notna(),:].drop('LotFrontage',axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lf_Xtrain = data[numerical_columns_lst].loc[data['LotFrontage'].notna(),:].drop('LotFrontage',axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[numerical_columns_lst].loc[data['LotFrontage'].notna(),:]['LotFrontage']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lf_Ytrain = data[numerical_columns_lst].loc[data['LotFrontage'].notna(),:]['LotFrontage']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data[numerical_columns_lst].loc[data['LotFrontage'].isna(),:].drop('LotFrontage',axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[numerical_columns_lst].loc[data['LotFrontage'].isna(),:].drop('LotFrontage',axis=1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "lf_Xtest = data[numerical_columns_lst].loc[data['LotFrontage'].isna(),:].drop('LotFrontage',axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.loc[data['LotFrontage'].isna(),:]['LotFrontage'].index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knr = KNeighborsRegressor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knr.fit(lf_Xtrain, lf_Ytrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "knr.predict(lf_Xtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.loc[data['LotFrontage'].isna(),'LotFrontage'] = knr.predict(lf_Xtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['LotFrontage'].isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[['LotFrontage']].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now all the columns in the numerical_col list have been deat with \n",
    "# we will cleaning the categorical column list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for cat in categorical_columns_lst:\n",
    "    if data[cat].isna().sum() >0:\n",
    "        print(cat, ' : ', data[cat].isna().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filling the na values in the Alley column with 'None' since it is a considered category in the datat description \n",
    "# But this assumption is dangerous since there are alot of na  values in this specific column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Alley'].fillna('None',inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to fill the na values of FirePlaceQu column we check if no of houses with 0 fire places is almost equal to the number of na values\n",
    "# in the FirePlaceQu column so we can fill na with 'None'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Fireplaces'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['FireplaceQu'].fillna('None',inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for cat in categorical_columns_lst:\n",
    "    if data[cat].isna().sum() >0:\n",
    "        print(cat, ' : ', data[cat].isna().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to fill the na values of PoolQC column we check if no of houses with Pool Area = 0 is almost equal to the number of na values\n",
    "# in the PoolQC column so we can fillna withj 'None'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['PoolArea'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we fill 2906 na values in the PoolQc column with a corresponding pool area = 0 with 'None'\n",
    "# we will check the remaing three na values separately"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.loc[data['PoolArea'] ==0,['PoolQC','PoolArea']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.loc[data['PoolArea'] ==0,['PoolQC']] = data.loc[data['PoolArea'] ==0,['PoolQC']].fillna('None')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['PoolQC'].isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "data.loc[data.PoolQC.isna(),['PoolQC','PoolArea']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.PoolQC.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.PoolQC.value_counts().index[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.PoolQC.fillna(data.PoolQC.value_counts().index[1],inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data[['PoolQC','PoolArea']].isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we fillna values in the Fence column with 'None' since none is a defined category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Fence'].fillna('None',inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for cat in categorical_columns_lst:\n",
    "    if data[cat].isna().sum() >0:\n",
    "        print(cat, ' : ', data[cat].isna().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of na values are not equal across the basement columns while they should be equal if the same houses have no basement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data.loc[data.BsmtFinType1.isna(),['BsmtQual','BsmtCond','BsmtExposure','BsmtFinType1','BsmtFinType2','BsmtFinSF1','BsmtFinSF2','BsmtUnfSF','TotalBsmtSF','BsmtFullBath','BsmtHalfBath']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data.loc[data.BsmtFinType1.isna(),['BsmtQual','BsmtCond','BsmtExposure','BsmtFinType1','BsmtFinType2']].isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we will all the na values above with 'None'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.loc[data.BsmtFinType1.isna(),['BsmtQual','BsmtCond','BsmtExposure','BsmtFinType1','BsmtFinType2']] = data.loc[data.BsmtFinType1.isna(),['BsmtQual','BsmtCond','BsmtExposure','BsmtFinType1','BsmtFinType2']].fillna('None')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data.loc[data.BsmtFinType1.isna(),['BsmtQual','BsmtCond','BsmtExposure','BsmtFinType1','BsmtFinType2']].isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we will check the remaining na values in the Basement features separately"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for cat in categorical_columns_lst:\n",
    "    if data[cat].isna().sum() >0:\n",
    "        print(cat, ' : ', data[cat].isna().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.loc[data.BsmtCond.isna(),['BsmtQual','BsmtCond','BsmtExposure','BsmtFinType1','BsmtFinType2']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we will fill the na values in the BSMTCond with the mode of the column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.BsmtCond.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.BsmtCond.fillna(data.BsmtCond.mode()[0], inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.loc[data.BsmtExposure.isna(),['BsmtQual','BsmtCond','BsmtExposure','BsmtFinType1','BsmtFinType2']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.BsmtExposure.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.BsmtExposure.fillna(data.BsmtExposure.mode()[0], inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.BsmtQual.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.BsmtQual.fillna(data.BsmtQual.mode()[0], inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.BsmtFinType2.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.BsmtFinType2.fillna(data.BsmtFinType2.mode()[0], inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for cat in categorical_columns_lst:\n",
    "    if data[cat].isna().sum() >0:\n",
    "        print(cat, ' : ', data[cat].isna().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.loc[data.MiscFeature.isna(),['MiscFeature','MiscVal']]['MiscVal'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we will fill the na values in MiscFeature with 'None'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.MiscFeature.fillna('None', inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.MiscFeature.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking the value counts and is na of the MiscVal Column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.MiscVal.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sns.heatmap(data.isna(),cmap = 'viridis')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now all the data have been cleaned, all the na values have been dealt with\n",
    "# let's search for numerical columns that are originally categorical:\n",
    "\n",
    "# MSSubClass : values are numerical but they represent categories\n",
    "# OverallQual : values are from 1 to 10 that represent the quality ascendingly\n",
    "# OverallCond : values are from 1 to 10 that represent the quality ascendingly\n",
    "# MoSold: values from 1 to 12\n",
    "# YrSold can be considered as a categorical feature only years from 2006 to 2010 are covered in the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.MoSold.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data.YrSold.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (12,7))\n",
    "sns.boxplot(x=train['MoSold'],y=np.log1p(train['SalePrice']))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we will divide features into three categories to start encoding or using get_dummies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify ordinal features from the description file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ordinal_features =['Utilities','ExterQual', 'ExterCond', 'BsmtQual','BsmtCond','BsmtFinType1','BsmtFinType2',\n",
    "#                   'HeatingQC','KitchenQual','FireplaceQu','GarageFinish','GarageQual','GarageCond','PoolQC','Fence']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#nominal_features = categorical_columns_lst.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for feat  in ordinal_features:\n",
    "#    nominal_features.remove(feat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data[nominal_features].nunique().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MSSubClass : values are numerical but they represent categories\n",
    "# OverallQual : values are from 1 to 10 that represent the quality ascendingly\n",
    "# OverallCond : values are from 1 to 10 that represent the quality ascendingly\n",
    "# MoSold: values from 1 to 12\n",
    "# YrSold can be considered as a categorical feature only years from 2006 to 2010 are covered in the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we will convert MSSubClass,MoSold, YrSold numerical features into strings so they can be identified as categories and not\n",
    "# numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.MSSubClass = data.MSSubClass.astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.MoSold = data.MoSold.astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.YrSold = data.YrSold.astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we will start encoding ordinal categorical features using Ordinal encoder from sklearn library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from sklearn.preprocessing import OrdinalEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for feat in ordinal_features:\n",
    "#    print(feat, ':', data[feat].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ut = ['ELO','NoSeWa','NoSewr','AllPub']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ex= ['Po','Fa','TA','Gd','Ex']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#q = ['None','Po','Fa','TA','Gd','Ex']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#bsmt_finType = ['None','Unf','LwQ','Rec','BLQ','ALQ','GLQ']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#h_q = ['Po','Fa','TA','Gd','Ex']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#k_q= ['Po','Fa','TA','Gd','Ex']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#g_f = ['None','Unf','RFn','Fin']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pool_q = ['None','Fa','TA','Gd','Ex']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fen_q = ['None','MnWw','GdWo', 'MnPrv',  'GdPrv']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ord_en = OrdinalEncoder(categories  = [ut , ex , ex , q , q , bsmt_finType , bsmt_finType , h_q , k_q , q , g_f , q , \n",
    "#                                       q , pool_q , fen_q ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data[ordinal_features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ord_en.fit_transform(data[ordinal_features])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data[ordinal_features] = ord_en.fit_transform(data[ordinal_features])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#data[ordinal_features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data[ordinal_features].info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now that ordinal categorical features have been encoded , we will encode other nominal category features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data[nominal_features].nunique().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from sklearn.preprocessing import OneHotEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ohe = OneHotEncoder(sparse = False)  # sparce = false is a must to get an array from the line code below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ohe.fit_transform(data[categorical_columns_lst]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ohe.fit_transform(data['YrSold'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ohe.fit_transform(data[nominal_features]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[categorical_columns_lst].info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data = pd.get_dummies(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical_columns_lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame({'skewness':data[numerical_columns_lst].skew(),\n",
    "                 'log':np.log(data[numerical_columns_lst]).skew(),\n",
    "                 'log1p':np.log1p(data[numerical_columns_lst]).skew(),\n",
    "                  'sqrt':np.sqrt(data[numerical_columns_lst]).skew(),\n",
    "                 'cbrt':np.cbrt(data[numerical_columns_lst]).skew(),\n",
    "                 'reip':np.reciprocal(data[numerical_columns_lst]).skew(),\n",
    "                 'square':np.square(data[numerical_columns_lst]).skew(),\n",
    "                 'cube':np.power(data[numerical_columns_lst],3).skew()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# log 1p is the best transformation method so we will go with it for all features with numerical columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[numerical_columns_lst] = np.log1p(data[numerical_columns_lst])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame({'skewness':data[numerical_columns_lst].skew(),\n",
    "                 'log':np.log(data[numerical_columns_lst]).skew(),\n",
    "                 'log1p':np.log1p(data[numerical_columns_lst]).skew(),\n",
    "                  'sqrt':np.sqrt(data[numerical_columns_lst]).skew(),\n",
    "                 'cbrt':np.cbrt(data[numerical_columns_lst]).skew(),\n",
    "                 'reip':np.reciprocal(data[numerical_columns_lst]).skew(),\n",
    "                 'square':np.square(data[numerical_columns_lst]).skew(),\n",
    "                 'cube':np.power(data[numerical_columns_lst],3).skew()})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Target column transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = train['SalePrice']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (20,7))\n",
    "plt.subplot(1,2,1)\n",
    "sns.distplot(target, fit = stats.norm)\n",
    "plt.xlabel('SalePrice')\n",
    "plt.subplot(1,2,2)\n",
    "sns.distplot(np.log(target), fit = stats.norm)\n",
    "plt.xlabel('log-transformed SalePrice')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_SalePrice = np.log(train.SalePrice)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler.fit(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_scaled = scaler.transform(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.DataFrame(df_scaled, index = data.index ,  columns = data.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now that data is ready to be processed by algorithms, we split back the combined data frame 'data' to train01 and test01 data sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train01 = data.loc[:train.index.max(),:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test01 = data.loc[train.index.max()+1:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.concat([train01,log_SalePrice],axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "s = setup(data = pd.concat([train01,log_SalePrice],axis=1),target='SalePrice')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from catboost import CatBoostRegressor\n",
    "from sklearn.linear_model import BayesianRidge, Ridge, OrthogonalMatchingPursuit\n",
    "from lightgbm import LGBMRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor, ExtraTreesRegressor\n",
    "from xgboost import XGBRegressor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### we start with the best performing model as per the pycaret run above which is CatBoost\n",
    "#### we will run fit the model on the training set and then cross validate it using kfold=10 also on the trainng set as it \n",
    "#### contains a target column which is log_SalePrice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cbr_base_m = CatBoostRegressor(verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cbr_base_m.fit(train01,log_SalePrice)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold,cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kf = KFold(n_splits=10)\n",
    "result_s = cross_val_score(cbr_base_m , train01 , log_SalePrice , scoring='neg_mean_squared_error' , cv=kf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16,10))\n",
    "sns.distplot(-result_s, bins = 20,kde = False )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(-results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.exp(np.sqrt(np.mean(-results)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Submission CatBoost Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### we make a cbr_predictions generated from the CatBoost Model above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cbr_predictions_log = cbr_base_m.predict(test01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cbr_predictions_log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cbr_predictions = np.exp(cbr_predictions_log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cbr_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_sample.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series(cbr_predictions,name='SalePrice')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission006 = pd.DataFrame(pd.concat([test['Id'],pd.Series(cbr_predictions,name='SalePrice')],axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission006.set_index('Id',inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission006"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission006.to_csv('E:/00-Data Science and Data Analysis/ML Projects/02-House Prices - Advanced Regression Techniques/dataset/submission006.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Submission Baysian Ridge Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "br01 = BayesianRidge()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "br01.fit(train01,log_SalePrice)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_br01 = cross_val_score(br01 , train01 , log_SalePrice , scoring='neg_mean_squared_error' , cv=kf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.exp(np.sqrt(-result_br01))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(np.exp(np.sqrt(-result_br01)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Submission bagging ensemble 8 models (equal weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {'catboost':CatBoostRegressor(verbose=0),'gbr':GradientBoostingRegressor(),'br':BayesianRidge(), 'lgbm':LGBMRegressor()\n",
    "         ,'r':Ridge(),'omp':OrthogonalMatchingPursuit(),'etr':ExtraTreesRegressor(),'xgboost':XGBRegressor()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name , model in models.items():\n",
    "    model.fit(train01,log_SalePrice)\n",
    "    print(name, ': trained')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = {}\n",
    "kf = KFold(n_splits=10)\n",
    "for name , model in models.items():\n",
    "    result = -cross_val_score(model , train01 , log_SalePrice , scoring='neg_mean_squared_error' , cv=kf)\n",
    "    results[name] = result   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name , result in results.items():\n",
    "    print('----',name,'----')\n",
    "    print('mean = ',np.mean(np.exp(np.sqrt(result))))\n",
    "    print('std  = ',np.std(np.exp(np.sqrt(result))))\n",
    "    print('----------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### combine predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_predictions = (   0.125* np.exp(models['catboost'].predict(test01))+\n",
    "                           0.125* np.exp(models['gbr'].predict(test01))+\n",
    "                           0.125* np.exp(models['br'].predict(test01))+\n",
    "                           0.125* np.exp(models['lgbm'].predict(test01))+\n",
    "                           0.125* np.exp(models['r'].predict(test01))+\n",
    "                           0.125* np.exp(models['omp'].predict(test01))+\n",
    "                           0.125* np.exp(models['etr'].predict(test01))+\n",
    "                           0.125* np.exp(models['xgboost'].predict(test01)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "combined_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission007 = pd.DataFrame(pd.concat([test['Id'],pd.Series(combined_predictions,name='SalePrice')],axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission007.set_index('Id',inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission007"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission007.to_csv('E:/00-Data Science and Data Analysis/ML Projects/02-House Prices - Advanced Regression Techniques/dataset/submission007.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### submission008 with custome ensemble weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### we are going to drop the etr and r modesl and put more weight to catboost model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_predictions_01 = (   0.25* np.exp(models['catboost'].predict(test01))+\n",
    "                           0.15* np.exp(models['gbr'].predict(test01))+\n",
    "                           0.15* np.exp(models['br'].predict(test01))+\n",
    "                           0.15* np.exp(models['lgbm'].predict(test01))+\n",
    "                           0.15* np.exp(models['omp'].predict(test01))+\n",
    "                           0.15* np.exp(models['xgboost'].predict(test01)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "combined_predictions_01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission008 = pd.DataFrame(pd.concat([test['Id'],pd.Series(combined_predictions_01,name='SalePrice')],axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission008.set_index('Id',inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission008"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission008.to_csv('E:/00-Data Science and Data Analysis/ML Projects/02-House Prices - Advanced Regression Techniques/dataset/submission008.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### submission009 with custom ensemble weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### we are going to drop the XGboost model and put more weight to catboost model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_009 = results.copy()\n",
    "results_009.pop('r')\n",
    "results_009.pop('etr')\n",
    "results_009.pop('xgboost')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_009"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum_mean=0\n",
    "sum_std=0\n",
    "for name , result in results_009.items():\n",
    "    sum_mean+=np.mean(np.exp(np.sqrt(result)))\n",
    "    sum_std+=np.std(np.exp(np.sqrt(result)))\n",
    "print(sum_mean)\n",
    "print(sum_std)\n",
    "for name , result in results_009.items():\n",
    "    print('----',name,'----')\n",
    "    print('mean = ',np.mean(np.exp(np.sqrt(result))))\n",
    "    print('std  = ',np.std(np.exp(np.sqrt(result))))\n",
    "    print('mean_ratio = ',np.mean(np.exp(np.sqrt(result)))/sum_mean)\n",
    "    print('std_ratio  = ',np.std(np.exp(np.sqrt(result)))/sum_std)\n",
    "    print('----------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_predictions_02 = (   0.30* np.exp(models['catboost'].predict(test01))+\n",
    "                           0.175* np.exp(models['gbr'].predict(test01))+\n",
    "                           0.175* np.exp(models['br'].predict(test01))+\n",
    "                           0.175* np.exp(models['lgbm'].predict(test01))+\n",
    "                           0.175* np.exp(models['omp'].predict(test01)) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "combined_predictions_02"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission009 = pd.DataFrame(pd.concat([test['Id'],pd.Series(combined_predictions_02,name='SalePrice')],axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission009.set_index('Id',inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission009"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission009.to_csv('E:/00-Data Science and Data Analysis/ML Projects/02-House Prices - Advanced Regression Techniques/dataset/submission009.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trying HyperParameter optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def br_objective(trial):\n",
    "    n_iter = trial.suggest_int('n_iter',50,600)\n",
    "    tol = trial.suggest_loguniform('tol',1e-8,10)\n",
    "    alpha_1 = trial.suggest_loguniform('alpha_1', 1e-8 , 10.0)\n",
    "    alpha_2 = trial.suggest_loguniform('alpha_2', 1e-8 , 10.0)\n",
    "    lambda_1 = trial.suggest_loguniform('lambda_1', 1e-8 , 10.0)\n",
    "    lambda_2 = trial.suggest_loguniform('lambda_2', 1e-8 , 10.0)\n",
    "    op_br = BayesianRidge(n_iter=n_iter,\n",
    "                          tol=tol,\n",
    "                          alpha_1=alpha_1,\n",
    "                          lambda_1=lambda_1,\n",
    "                          lambda_2=lambda_2)\n",
    "    \n",
    "    op_br.fit(train01 , log_SalePrice)\n",
    "    \n",
    "    cv_scores =  np.exp(np.sqrt(-cross_val_score(op_br , train01 , log_SalePrice , scoring='neg_mean_squared_error' , cv=kf)))\n",
    "    \n",
    "    return np.mean(cv_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "study = optuna.create_study(direction = 'minimize')\n",
    "study.optimize(br_objective , n_trials=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "study.best_params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### parameters below for catboost, lgbm and ridge models are taken from Gabriel's Video\n",
    "### BayesianRidge hyperparameters are taken from the run above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "br_param = {'n_iter': 492,\n",
    "     'tol': 0.0018686052265045683,\n",
    "     'alpha_1': 0.00623854306738154,\n",
    "     'alpha_2': 1.9861300153063412e-07,\n",
    "     'lambda_1': 9.956444983866877,\n",
    " 'lambda_2': 2.5815902917188803e-07\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "catboost_param= {'iterations':6000,\n",
    "    'learning_rate':0.005,\n",
    "    'depth':4,\n",
    "    'l2_leaf_reg':1,\n",
    "    'eval_metric':'RMSE',\n",
    "    'early_stopping_rounds':200,\n",
    "    'random_seed':42\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgbm_param = {  'num_leaves': 39,\n",
    "    'max_depth': 2,\n",
    "    'learning_rate': 0.13705339989856127,\n",
    "    'n_estimators': 273    \n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We will use Optuna to optimize hyperparameters for LGBM, GBR and OMP models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lgbm_objective(trial):\n",
    "    num_leaves   = trial.suggest_int('num_leaves',20,200)\n",
    "    \n",
    "    op_lgbm = LGBMRegressor(        max_depth = 75,\n",
    "                                    num_leaves = num_leaves,\n",
    "                                    learning_rate= 0.10019531353013397,\n",
    "                                    n_estimators= 173,\n",
    "                                    min_child_weight = 0.006045642013402679,\n",
    "                                    min_child_samples = 53\n",
    "                                        )\n",
    "    \n",
    "    op_lgbm.fit(train01 , log_SalePrice)\n",
    "    \n",
    "    cv_scores =  np.exp(np.sqrt(-cross_val_score(op_lgbm , train01 , log_SalePrice , scoring='neg_mean_squared_error' , cv=kf)))\n",
    "    \n",
    "    return np.mean(cv_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "lgbm_study = optuna.create_study(direction = 'minimize')\n",
    "lgbm_study.optimize(lgbm_objective , n_trials=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgbm_study.best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgbm02 =LGBMRegressor( **lgbm_param) \n",
    "lgbm02.fit(train01 , log_SalePrice)  \n",
    "lgbm02_cv_scores =  np.exp(np.sqrt(-cross_val_score(lgbm02 , train01 , log_SalePrice , scoring='neg_mean_squared_error' , cv=kf)))\n",
    "np.mean(lgbm02_cv_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Best parameters above result in less acurate model than gabriel's parameters so we will use Gabriel's Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gbr_objective(trial):\n",
    "    learning_rate= trial.suggest_loguniform('learning_rate',1e-8,10)\n",
    "    n_estimators= trial.suggest_int('n_estimators',5,500)\n",
    "    min_samples_split= trial.suggest_int('min_samples_split',2,50)\n",
    "    min_samples_leaf= trial.suggest_int('min_samples_leaf',2,50)\n",
    "    max_depth=trial.suggest_int('max_depth',1,50)\n",
    "    tol=trial.suggest_loguniform('tol',1e-8,10)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    op_gbr = GradientBoostingRegressor(    learning_rate= learning_rate,\n",
    "                                            n_estimators= n_estimators,\n",
    "                                            min_samples_split= min_samples_split,\n",
    "                                            min_samples_leaf= min_samples_leaf,\n",
    "                                            max_depth=max_depth,\n",
    "                                            tol=tol\n",
    "                                        )\n",
    "    \n",
    "    op_gbr.fit(train01 , log_SalePrice)\n",
    "    \n",
    "    cv_scores =  np.exp(np.sqrt(-cross_val_score(op_gbr , train01 , log_SalePrice , scoring='neg_mean_squared_error' , cv=kf)))\n",
    "    \n",
    "    return np.mean(cv_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "gbr_study = optuna.create_study(direction = 'minimize')\n",
    "gbr_study.optimize(gbr_objective , n_trials=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbr_study.best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbr_param = {'learning_rate': 0.07349784987251627,\n",
    " 'n_estimators': 458,\n",
    " 'min_samples_split': 32,\n",
    " 'min_samples_leaf': 7,\n",
    " 'max_depth': 3,\n",
    " 'tol': 0.17057501671876868}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def omp_objective(trial):\n",
    "    tol=trial.suggest_loguniform('tol',1e-8,10)\n",
    "    op_omp = OrthogonalMatchingPursuit(tol = tol)\n",
    "    \n",
    "    op_omp.fit(train01 , log_SalePrice)\n",
    "    \n",
    "    cv_scores =  np.exp(np.sqrt(-cross_val_score(op_omp , train01 , log_SalePrice , scoring='neg_mean_squared_error' , cv=kf)))\n",
    "    \n",
    "    return np.mean(cv_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "omp_study = optuna.create_study(direction = 'minimize')\n",
    "omp_study.optimize(omp_objective , n_trials=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "omp_study.best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "omp_param = {'tol': 9.941573492480998}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#the above value results in a less accurate model than with the default hyper parameters withn the omp model in scikit learn\n",
    "#so we will go with the defaults"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_ensemble = {'catboost':CatBoostRegressor(verbose=0),'gbr':GradientBoostingRegressor(),'br':BayesianRidge(), 'lgbm':LGBMRegressor()\n",
    "         ,'omp':OrthogonalMatchingPursuit()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### final catboost submission with the optimized hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "catboost_final = CatBoostRegressor(**catboost_param)\n",
    "catboost_final.fit(train01,log_SalePrice)\n",
    "catboost_log_pred = catboost_final.predict(test01)\n",
    "catboost_pred = np.exp(catboost_log_pred)\n",
    "catboost_predictions = pd.DataFrame(pd.concat([test['Id'],pd.Series(catboost_pred,name='SalePrice')],axis=1))\n",
    "catboost_predictions.set_index('Id',inplace=True)\n",
    "catboost_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "catboost_predictions.to_csv('E:/00-Data Science and Data Analysis/ML Projects/02-House Prices - Advanced Regression Techniques/dataset/submission010-catboost_predictions.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### final GradientBoostingRegression Model submission with the optimized hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbr_final = GradientBoostingRegressor(**gbr_param)\n",
    "gbr_final.fit(train01,log_SalePrice)\n",
    "gbr_log_pred = gbr_final.predict(test01)\n",
    "gbr_pred = np.exp(gbr_log_pred)\n",
    "gbr_predictions = pd.DataFrame(pd.concat([test['Id'],pd.Series(gbr_pred,name='SalePrice')],axis=1))\n",
    "gbr_predictions.set_index('Id',inplace=True)\n",
    "gbr_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbr_predictions.to_csv('E:/00-Data Science and Data Analysis/ML Projects/02-House Prices - Advanced Regression Techniques/dataset/submission011-gbr_predictions.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### final Bayesian Ridge regression Model submission with the optimized hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "br_final = BayesianRidge(**br_param)\n",
    "br_final.fit(train01,log_SalePrice)\n",
    "br_log_pred = br_final.predict(test01)\n",
    "br_pred = np.exp(br_log_pred)\n",
    "br_predictions = pd.DataFrame(pd.concat([test['Id'],pd.Series(br_pred,name='SalePrice')],axis=1))\n",
    "br_predictions.set_index('Id',inplace=True)\n",
    "br_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "br_predictions.to_csv('E:/00-Data Science and Data Analysis/ML Projects/02-House Prices - Advanced Regression Techniques/dataset/submission012-br_predictions.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### final LGBM regression Model submission with the optimized hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgbm_final = LGBMRegressor(**lgbm_param)\n",
    "lgbm_final.fit(train01,log_SalePrice)\n",
    "lgbm_log_pred = lgbm_final.predict(test01)\n",
    "lgbm_pred = np.exp(lgbm_log_pred)\n",
    "lgbm_predictions = pd.DataFrame(pd.concat([test['Id'],pd.Series(lgbm_pred,name='SalePrice')],axis=1))\n",
    "lgbm_predictions.set_index('Id',inplace=True)\n",
    "lgbm_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgbm_predictions.to_csv('E:/00-Data Science and Data Analysis/ML Projects/02-House Prices - Advanced Regression Techniques/dataset/submission013-lgbm_predictions.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### final Orthogonal Matching Pursuit regression Model submission with the default hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "omp_final = OrthogonalMatchingPursuit(**omp_param)\n",
    "omp_final.fit(train01,log_SalePrice)\n",
    "omp_log_pred = omp_final.predict(test01)\n",
    "omp_pred = np.exp(omp_log_pred)\n",
    "omp_predictions = pd.DataFrame(pd.concat([test['Id'],pd.Series(omp_pred,name='SalePrice')],axis=1))\n",
    "omp_predictions.set_index('Id',inplace=True)\n",
    "omp_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "omp_predictions.to_csv('E:/00-Data Science and Data Analysis/ML Projects/02-House Prices - Advanced Regression Techniques/dataset/submission014-omp_predictions.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bagging ensemble for 5 models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_predictions_03 = (   0.30* catboost_pred+\n",
    "                            0.175* gbr_pred+\n",
    "                           0.175* br_pred+\n",
    "                           0.175* lgbm_pred+\n",
    "                           0.175* omp_pred )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_predictions_03 = pd.DataFrame(pd.concat([test['Id'],pd.Series(combined_predictions_03,name='SalePrice')],axis=1))\n",
    "combined_predictions_03.set_index('Id',inplace=True)\n",
    "combined_predictions_03.to_csv('E:/00-Data Science and Data Analysis/ML Projects/02-House Prices - Advanced Regression Techniques/dataset/submission015-combined_predictions_03.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bagging ensemble for 4 models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_predictions_04 = (   0.25* catboost_pred+\n",
    "                            0.25* gbr_pred+\n",
    "                            0.25* br_pred+\n",
    "                            0.25* lgbm_pred )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_predictions_04 = pd.DataFrame(pd.concat([test['Id'],pd.Series(combined_predictions_04,name='SalePrice')],axis=1))\n",
    "combined_predictions_04.set_index('Id',inplace=True)\n",
    "combined_predictions_04.to_csv('E:/00-Data Science and Data Analysis/ML Projects/02-House Prices - Advanced Regression Techniques/dataset/submission016-combined_predictions_04.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bagging ensemble for 4 models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_predictions_05 = (   0.35* catboost_pred+\n",
    "                            0.25* gbr_pred+\n",
    "                            0.15* br_pred+\n",
    "                            0.25* lgbm_pred )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_predictions_05 = pd.DataFrame(pd.concat([test['Id'],pd.Series(combined_predictions_05,name='SalePrice')],axis=1))\n",
    "combined_predictions_05.set_index('Id',inplace=True)\n",
    "combined_predictions_05.to_csv('E:/00-Data Science and Data Analysis/ML Projects/02-House Prices - Advanced Regression Techniques/dataset/submission017-combined_predictions_05.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bagging ensemble for 5 models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_predictions_06 = (   0.36* catboost_pred+\n",
    "                            0.16* gbr_pred+\n",
    "                            0.16* br_pred+\n",
    "                            0.16* lgbm_pred+\n",
    "                            0.16* omp_pred )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_predictions_06 = pd.DataFrame(pd.concat([test['Id'],pd.Series(combined_predictions_06,name='SalePrice')],axis=1))\n",
    "combined_predictions_06.set_index('Id',inplace=True)\n",
    "combined_predictions_06.to_csv('E:/00-Data Science and Data Analysis/ML Projects/02-House Prices - Advanced Regression Techniques/dataset/submission018-combined_predictions_06.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bagging ensemble for 5 models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_predictions_07 = (   0.30* catboost_pred+\n",
    "                            0.15* gbr_pred+\n",
    "                            0.25* br_pred+\n",
    "                            0.15* lgbm_pred+\n",
    "                            0.15* omp_pred )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_predictions_07 = pd.DataFrame(pd.concat([test['Id'],pd.Series(combined_predictions_07,name='SalePrice')],axis=1))\n",
    "combined_predictions_07.set_index('Id',inplace=True)\n",
    "combined_predictions_07.to_csv('E:/00-Data Science and Data Analysis/ML Projects/02-House Prices - Advanced Regression Techniques/dataset/submission019-combined_predictions_07.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bagging ensemble for 5 models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_predictions_08 = (   0.40* catboost_pred+\n",
    "                            0.15* gbr_pred+\n",
    "                            0.15* br_pred+\n",
    "                            0.15* lgbm_pred+\n",
    "                            0.15* omp_pred )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_predictions_08 = pd.DataFrame(pd.concat([test['Id'],pd.Series(combined_predictions_08,name='SalePrice')],axis=1))\n",
    "combined_predictions_08.set_index('Id',inplace=True)\n",
    "combined_predictions_08.to_csv('E:/00-Data Science and Data Analysis/ML Projects/02-House Prices - Advanced Regression Techniques/dataset/submission020-combined_predictions_08.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
